#-----------------------------------------------------------------------------------------------------------------------
# Analysis_Chp1 (2020)
# Title: Merge tables and rechunk
# del Giorgio, Olivia (https://github.com/odelgi)
# Messager, Mathis (https://github.com/messamat)

# Assess what livelihoods and locations access index has been computed, create access index rasters,
# assess computation speed and chunk groups of locations to compute access index for.
# Chunking allows analysis to be run in parallel within limited periods of time.

#Structure of analysis
# A. UTILITY FUNCTIONS: generic functions used in this code
#
# B. ANALYSIS FUNCTIONS: Function to merge access tables, create access raster, and prepare next processing batch.
#
# C. MAIN ANALYSIS: Run analysis first for a subset of areas recognized as developed/settled and then run analysis for
#                   entire department of Pellegrini
#
#-----------------------------------------------------------------------------------------------------------------------

# Import system modules
import arcpy
from arcpy.sa import *
import numpy as np
import os #Module for operating system operations (e.g. os.path.exists)
from shutil import copyfile
from collections import defaultdict
import time
import pandas as pd
import math
from accesscalc_parallel import *

###----------------------------------- A. UTILITY FUNCTIONS ---------------------------------------------------------###
def groupindexing(grouplist, chunknum):
    # Set of groups
    # Chunk size to divide groups into
    x = np.array(grouplist)
    bin_step = max(grouplist) / chunknum
    bin_edges = np.arange(min(i for i in grouplist if i is not None),
                          max(i for i in grouplist if i is not None) + bin_step,
                          bin_step)
    bin_number = bin_edges.size - 1
    cond = np.zeros((x.size, bin_number), dtype=bool)
    for i in range(bin_number):
        cond[:, i] = np.logical_and(bin_edges[i] <= x,
                                    x < bin_edges[i + 1])
    return [list(x[cond[:, i]]) for i in range(bin_number)]

def tabmerge_dict(tablist):
    """
    Merge list of zonal statistic mean tables into a dictionary.

    Args:
        tablist (iterator): list of paths to arcpy cursor-readable tables
    Returns:
        dictionary: key is "Value", value is "MEAN" across all zonal statistics tables
    """
    outdict = {}
    for tab in tablist:
        print(tab)
        for row in arcpy.da.SearchCursor(tab, ['Value', 'MEAN']):
            outdict[row[0]] = row[1]
    return(outdict)

###----------------------------------- B. ANALYSIS FUNCTION ---------------------------------------------------------###
def mergetables_rechunk(rootdir, in_pointstoprocess, analysis_years, in_statsdir, out_chunkdir, out_formatdir,
                        linearindexing=False, in_llhood=None):
    """
    Merge access tables, create access raster, and prepare next processing batch.

    To be run in parallel, accesscalc_parallel.py needs as inputs a directory containing all the locations around which
    to compute access separated into separate workspaces, each containing a subset of the locations and all the required
    input data and scripts to run the access analysis on an individual processing core. This allows the analysis to be
    parallelized with each core having its own input data and output locations.

    This function is not very modulable but runs through the steps of:
        - Re-define/create paths to access data and results from full analysis
        - For each livelihood:
            - Compile all access statistics tables generated by running accesscalc_parallel.py
            - Create a raster of access for each llhood and year by aggregating all tables, and while doing that, assess
                which locations still need to be processed (for those livelihoods whose calculations have been started)
            - Run access analysis on 10 groups of locations to assess the number of chunks to divide all locations into
            for each livelihood so that each chunk takes the same amount of time to process.
            - Divide all locations to process into separate chunks. For each chunk; create a directory with input data
                and scripts to run accesscalc_parallel.py (see requirements in documentation).

    Args:
        rootdir (str): project directory
        in_pointstoprocess (feature class): locations for which to compute mean access index in surrounding area
        analysis_years (list; int, or str): years to run analysis for
        in_statsdir (str): directory which contains table outputs from accesscalc_parallel.py
                           (can contain multiple livelihoods and years, must respect file naming convention)
        out_chunkdir (str): root directory to output chunk directories which will be used to run accesscalc_parallel.py
        out_formatdir (str): name of directory within which to group chunk directories. out_formatdir will be written within out_chunkdir
        linearindexing (bool): Whether to try to process groups in order
                               (if some groups have already been procssed. can lead to bugs - should keep as default).
                               Default: False
        in_llhood (list or str)= livelihoods to run analysis for. If left undetermined or None,
                                 will process all livelihoods in os.path.join(datadir, 'Weighting_scheme.xlsx')

    Returns:
        None
        Side effect:
            - Access rasters in rootdir/results/Analysis_Chp1_W1/W1_3030/Access_W1_3030 for each livelihood and year
            - Chunked and formatted directory to run accesscalc_parallel.py on. Written in out_chunkdir and named based
              on out_formatdir argument.
    """

    #Make sure analysis_years is list of strings
    if isinstance(analysis_years, str) or isinstance(analysis_years, int):
        analysis_years = [analysis_years]
    analysis_years = [str(yr) for yr in analysis_years]

    #Repreate paths
    datadir = os.path.join(rootdir, 'data')
    resdir = os.path.join(rootdir, 'results')

    #Make sure that output directory exists. If not, create it
    pathcheckcreate(out_chunkdir)

    #Get table with livelihoods, buffer radii, etc.
    weighting_table = os.path.join(datadir, 'Weighting_scheme.xlsx')
    weightingpd = pd.read_excel(weighting_table, sheetname='Weighting_1')

    #Format livelihoods to process
    if in_llhood is None:
        livelihoods = weightingpd['Livelihood'].tolist()
        livelihoods.remove('Combined_livelihood')
    else:
        livelihoods = in_llhood
        if isinstance(livelihoods, str):
            livelihoods = [livelihoods]

    ### ------- Re-define/create paths ---------------------------------------------------------------------------------
    basemapgdb = os.path.join(resdir, "Base_layers_Pellegrini", "Basemaps_UTM20S.gdb")
    pelleras = os.path.join(basemapgdb, "Pellegri_department_UTM20S")
    forestoutdir = os.path.join(resdir, 'Base_layers_Pellegrini/Forest_Hansen.gdb')
    barrierweight_outras = {}; forestyearly = {}; bufferad = {}; costtab_outgdb = {}
    access_outgdb = {}; access_outras = {}

    for llhood in livelihoods:
        print(llhood)
        bufferad[llhood] = float(weightingpd.loc[weightingpd['Livelihood'] == llhood, 'Buffer_max_rad'])
        outllhood_gdb = os.path.join(resdir,'Analysis_Chp1_W1','W1_3030','Barrier_weighting_1_3030','{}_bw1_3030.gdb'.format(llhood))
        for year in analysis_years:
            barrierweight_outras[llhood+year] = os.path.join(outllhood_gdb, '{0}_bw1_{1}'.format(llhood, year))
            forestyearly[year] = os.path.join(forestoutdir, 'Hansen_GFC_v16_treecover{}'.format(year))
            costtab_outgdb[llhood+year] = os.path.join(resdir, 'Analysis_Chp1_W1',
                                                       'W1_3030', 'Cost_distance_W1_3030',
                                                       'Cost_distance_{}_w1'.format(llhood),
                                                       'CD_{0}_{1}_w1.gdb'.format(llhood, year))

            access_outgdb[llhood] = os.path.join(resdir, 'Analysis_Chp1_W1', 'W1_3030', 'Access_W1_3030',
                                                 'Access_W1_{0}'.format(llhood), 'Access_W1_{0}.gdb'.format(llhood))
            pathcheckcreate(access_outgdb[llhood])
            # Path of output access raster
            access_outras[llhood + year] = os.path.join(access_outgdb[llhood],
                                                        'accessras_W1_{0}{1}'.format(llhood, year))
    
    # LOOP: for each livelihood, for each group, create separate folder-points-buffers to run cost-distance in parallel on HPC
    ### ------- Get all processed tables -----------------------------------------------------------------------------------
    tablist = getfilelist(dir=in_statsdir, repattern=".*[.]dbf$", gdbf = False, nongdbf = True)
    tablist.extend(getfilelist(dir=in_statsdir, gdbf = True, nongdbf = False))
    
    tables_pd = pd.concat([pd.Series(tablist),
                           pd.Series(tablist).apply(lambda x: os.path.splitext(os.path.split(x)[1])[0]).
                          str.split('_', expand=True)],
                          axis=1)
    tables_pd.columns = ['path', 'dataset', 'llhood1', 'llhood2', 'year', 'weighting', 'group']
    tables_pd['llhood'] = tables_pd['llhood1'] + '_' + tables_pd['llhood2']
    tables_pd = tables_pd.drop(labels=['llhood1', 'llhood2'], axis=1)
    
    # processed_pd = tables_pd.groupby(['llhood', 'group']).filter(lambda x: x['year'].nunique() == 3).\
    #     drop_duplicates(subset=['llhood', 'group'])
    
    ### ------ Create a raster of access for each llhood and year by aggregating all tables (yielding an access value for each pixel-point) ---------
    refraster = pelleras
    fishgroupstoprocess = defaultdict(set)
    
    # Iterate over each livelihood
    for llhood in tables_pd['llhood'].unique():
        #if processing that llhood
        if llhood in livelihoods:
            #Create a field that records whether all analysis years have been processed.
            # If all analysis years have been processed, it has a value of None, if not, it has a value of 0
            accessfield_toprocess = 'access{0}_toprocess'.format(llhood, year)
            print('Create {} field'.format(accessfield_toprocess))
            if accessfield_toprocess in [f.name for f in arcpy.ListFields(in_pointstoprocess)]:
                arcpy.DeleteField_management(in_pointstoprocess, accessfield_toprocess)
            arcpy.AddField_management(in_table=in_pointstoprocess, field_name= accessfield_toprocess, field_type='SHORT')

            # Iterate over each year
            for year in tables_pd['year'].unique():
                #if processing that year
                if year in analysis_years:
                    # Perform analysis only if output raster doesn't exist
                    if not arcpy.Exists(access_outras[llhood+year]):
                        print("Processing {}...".format(access_outras[llhood+year]))

                        # Aggregate values across all pixels-points for that livelihood-year
                        print('Aggregating zonal statistics tables...')
                        merged_dict = tabmerge_dict(tables_pd.loc[(tables_pd['llhood'] == llhood) &
                                                                  (tables_pd['year'] == year), 'path'])

                        if len(merged_dict) > 0:
                            # Join all statistics tables of access to in_pointstoprocess (a point for each 30x30 m pixel in Pellegrini department)
                            print('Joining tables to points...')

                            accessfield = 'access{0}{1}'.format(llhood, year)
                            if not accessfield in [f.name for f in arcpy.ListFields(in_pointstoprocess)]:
                                print('Create {} field'.format(accessfield))
                                arcpy.AddField_management(in_table=in_pointstoprocess, field_name=accessfield, field_type='FLOAT')

                            with arcpy.da.UpdateCursor(in_pointstoprocess, ['pointid', accessfield,  accessfield_toprocess,
                                                                            'group{}'.format(llhood)]) as cursor:
                                x = 0
                                for row in cursor:
                                    if x % 100000 == 0:
                                        print(x)
                                    if row[0] in merged_dict:
                                        row[1] = merged_dict[row[0]]
                                    else:
                                        row[2] = 1
                                        fishgroupstoprocess[llhood].add(row[3])
                                        print('pointid {} was not found in dictionary'.format(row[0]))
                                    cursor.updateRow(row)
                                    x += 1

                            # Convert points back to raster
                            #if len(merged_dict) == x:
                            print('Converting points to raster...')
                            arcpy.env.snapRaster = arcpy.env.extent = refraster
                            arcpy.PointToRaster_conversion(in_features=in_pointstoprocess,
                                                           value_field=accessfield,
                                                           cellsize=refraster,
                                                           out_rasterdataset=access_outras[llhood+year])
                            arcpy.ClearEnvironment("extent")
                            arcpy.ClearEnvironment("snapRaster")
                        else:
                            print('No zonal statistics available for that livelihood for that year...')

                    else:
                        print('{} already exists...'.format(access_outras[llhood+year]))

    ### ------ Run analysis on 10 groups to check speed ----- ####
    testdir = os.path.join(out_chunkdir, 'testdir')
    if os.path.isdir(testdir):
        arcpy.Delete_management(testdir)
    pathcheckcreate(testdir)
    grp_process_time = defaultdict(float)
    
    for llhood in livelihoods:
        print('Assessing access calculation run time for {}...'.format(llhood))
    
        if 'group{}'.format(llhood) not in [i.name for i in arcpy.Describe(in_pointstoprocess).indexes]:
            print('Adding index to in_pointstoprocess...')
            arcpy.AddIndex_management(in_pointstoprocess, fields='group{}'.format(llhood),
                                      index_name='group{}'.format(llhood))  # Add index to speed up querying
    
        if ((llhood not in fishgroupstoprocess) and
                (not all(arcpy.Exists(access_outras[llhood+y]) for y in analysis_years))):
            print('Getting groups...')
            fishgroupstoprocess[llhood] = {row[0] for row in arcpy.da.SearchCursor(in_pointstoprocess,
                                                                                   'group{}'.format(llhood))}
    
        # Output points for 10 groups for each livelihood
        for group in list(fishgroupstoprocess[llhood])[0:10]:
            print(group)
    
            outpoints = os.path.join(testdir, 'testpoints_{0}_{1}_{2}.shp').format(llhood, int(bufferad[llhood]), group)
            if not arcpy.Exists(outpoints):
                # Subset points based on group (so that their buffers don't overlap) and only keep points that overlap study area
                arcpy.MakeFeatureLayer_management(in_features=in_pointstoprocess, out_layer='subpoints{}'.format(group),
                                                  where_clause='({0} = {1}) AND ({2} = 1)'.format(
                                                      'group{}'.format(llhood), group, accessfield_toprocess))
                arcpy.CopyFeatures_management('subpoints{}'.format(group), outpoints)
    
            # Test time that each group takes to process for each livelihood
            inbw = {yr: barrierweight_outras['{0}{1}'.format(llhood, yr)] for yr in analysis_years}
    
            tic = time.time()
            # Get subpoints
            accesscalc(inllhood=llhood,
                        ingroup=group,
                        inpoints=outpoints,
                        inbuffer_radius=bufferad[llhood],
                        inyears=analysis_years,
                        inbarrierweight_outras=inbw,
                        inforestyearly=forestyearly,
                        costtab_outdir=testdir)
            toc = time.time()
            print(toc - tic)
            grp_process_time[llhood] = grp_process_time[llhood] + (toc - tic) / 10.0
    
    ### ------ Compute number of chunks to divide each livelihood in to process each chunk with equal time ------###
    numcores = 14  # Number of chunks to divide processing into
    maxdays = 1 #Max number of days that processes can be run at a time

    #Assess amount of time reguired and number of chunks
    totaltime = sum([grp_process_time[llhood] * len(fishgroupstoprocess[llhood]) for llhood in livelihoods])
    print('Total processing times among {0} cores: {1} days...'.format(
        numcores, totaltime/float(3600.0*24*numcores))) #Total time if process is divided into numcores chunks at same speed
    numchunks = math.ceil(totaltime / float(3600.0 * 24 * maxdays))
    print('Total number of chunks for each to be processed within {0} days among {1} cores: {2}...'.format(
        maxdays, numcores, numchunks))
    
    ### ------ Assign groups to chunks ------###
    llhood_chunks = {}
    formatdir_data = os.path.join(out_formatdir, 'data')
    formatdir_results = os.path.join(out_formatdir, 'results')
    formatdir_src = os.path.join(out_formatdir, 'src')
    pathcheckcreate(formatdir_data, verbose=True)
    pathcheckcreate(formatdir_results, verbose=True)
    pathcheckcreate(formatdir_src, verbose=True)
    
    #Copy processing file to directory
    in_processingscript = os.path.join(rootdir, 'src', 'Chap1_Analysis1', 'accesscalc_parallel.py')
    out_processingscript = os.path.join(formatdir_src, 'accesscalc_parallel.py')
    copyfile(in_processingscript, out_processingscript)
    
    for llhood in livelihoods:
        if len(fishgroupstoprocess[llhood]) > 0:
            print(llhood)
            llhood_chunks[llhood] = math.ceil(numchunks * grp_process_time[llhood] * len(fishgroupstoprocess[llhood]) / totaltime)
            print('    Number of chunks to divide {0} groups into: {1}...'.format(
                llhood, llhood_chunks[llhood]))

            if linearindexing == True:
                groupchunklist = groupindexing(grouplist=list(fishgroupstoprocess[llhood]), chunknum=llhood_chunks[llhood])
            else:
                interval = int(math.ceil(len(fishgroupstoprocess[llhood])/ float(numchunks)))
                groupchunklist = [list(fishgroupstoprocess[llhood])[i:(i + interval)] for i
                                  in range(0, len(fishgroupstoprocess[llhood]), interval)]
    
            #Output points and ancillary data to chunk-specific gdb
            for chunk in range(0, len(groupchunklist)):
                print(chunk)
                outchunkgdb = os.path.join(formatdir_data, '{0}{1}_{2}.gdb'.format(llhood, int(bufferad[llhood]), chunk))
                if not (arcpy.Exists(outchunkgdb)):
                    pathcheckcreate(outchunkgdb, verbose=True)
                    outchunkpoints= os.path.join(outchunkgdb, 'subpoints{0}{1}_{2}'.format(llhood, int(bufferad[llhood]), chunk))

                    print('Copying points...')
                    if len(groupchunklist[chunk])>0:
                        print(len(groupchunklist[chunk]))
                        arcpy.CopyFeatures_management(
                            arcpy.MakeFeatureLayer_management(
                                in_features=in_pointstoprocess, out_layer='pointslyr',
                                where_clause='(group{0} IN {1}) AND ({2} = 1)'.format(
                                    llhood, tuple(groupchunklist[chunk]), accessfield_toprocess)
                            ), #[i for i in groupchunklist[chunk] if i is not None]
                            outchunkpoints)

                        print('Copying ancillary data...')
                        for yr in analysis_years:
                            #Copy barrier raster
                            arcpy.CopyRaster_management(barrierweight_outras[llhood + yr],
                                                        os.path.join(outchunkgdb, os.path.split(barrierweight_outras[llhood + yr])[1]))
                            #Copy forest cover
                            if llhood == 'Charcoal_production':
                                arcpy.CopyRaster_management(forestyearly[yr],
                                                            os.path.join(outchunkgdb, os.path.split(forestyearly[yr])[1]))
                else:
                    print('{} already exists...'.format(outchunkgdb))
        else:
            print('All groups for {} have already been processed...'.format(llhood))

###----------------------------------- C. MAIN ANALYSIS -------------------------------------------------------------###

if __name__ == '__main__':
    ### Set environment settings ###
    # https://pro.arcgis.com/en/pro-app/arcpy/classes/env.htm
    arcpy.env.overwriteOutput = 'True'
    arcpy.CheckOutExtension("Spatial")

    #### Directory structure ###
    rootdir = os.path.dirname(os.path.abspath(__file__)).split('\\src')[0]
    datadir = os.path.join(rootdir, 'data')
    resdir = os.path.join(rootdir, 'results')

    llhoodbuffer_outdir = os.path.join(resdir, 'Analysis_Chp1_W1',
                                       'Buffers_W1.gdb')  # Output buffer path for each livelihood
    pellepoints = os.path.join(llhoodbuffer_outdir, 'pellefishpoints')

    #Process subset of areas for assessing access for households that were interviewed
    chunkdir_hhdat = os.path.join(resdir, 'Analysis_Chp1_W1', 'inputdata_hh', 'datapts.gdb')
    pellehhpts = os.path.join(chunkdir_hhdat, 'pellepoints_households')

    mergetables_rechunk(rootdir=rootdir,
                        in_pointstoprocess=pellehhpts,
                        analysis_years=['2000', '2010', '2018'],
                        in_statsdir=os.path.join(resdir, 'Analysis_Chp1_W1/W1_3030/Cost_distance_W1_3030'),
                        out_chunkdir=os.path.join(resdir, 'Analysis_Chp1_W1', 'inputdata_hh'),
                        out_formatdir=os.path.join(resdir, 'Analysis_Chp1_W1', 'inputdata_hh',
                                                   'Householdschunks_input{}'.format(time.strftime("%Y%m%d"))),
                        linearindexing=False)

    #Process locations for assessing access in specific communities
    chunkdir_commu = os.path.join(resdir, 'Analysis_Chp1_W1', 'inputdata_communities')
    chunkdir_commudat = os.path.join(chunkdir_commu, 'datapts.gdb')
    commupts = os.path.join(chunkdir_commudat, 'pellepoints_developedareas')

    mergetables_rechunk(rootdir=rootdir,
                        in_pointstoprocess=commupts,
                        analysis_years=['2000', '2010', '2018'],
                        in_statsdir=os.path.join(resdir, 'Analysis_Chp1_W1/W1_3030/Cost_distance_W1_3030'),
                        out_chunkdir=chunkdir_commu,
                        out_formatdir=os.path.join(chunkdir_commu, 'Communitychunks_input{}'.format(time.strftime("%Y%m%d"))),
                        linearindexing=False,
                        in_llhood="Charcoal_production")

    #Process full extent
    chunkdir_HPC = os.path.join(resdir, 'Analysis_Chp1_W1', 'inputdata_HPC')
    mergetables_rechunk(rootdir = rootdir,
                        in_pointstoprocess = pellepoints,
                        analysis_years= ['2000', '2010', '2018'],  # Years for analysis
                        in_statsdir = os.path.join(resdir, 'Analysis_Chp1_W1/W1_3030/Cost_distance_W1_3030'),
                        out_chunkdir = chunkdir_HPC,
                        out_formatdir = os.path.join(chunkdir_HPC, 'White_input{}'.format(time.strftime("%Y%m%d"))),
                        linearindexing=False,
                        in_llhood='Caprine_livestock')
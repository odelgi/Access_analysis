#-----------------------------------------------------------------------------------------------------------------------
# Analysis_Chp1 (2020)
# Title: Access calculation
# del Giorgio, Olivia (https://github.com/odelgi)
# Messager, Mathis (https://github.com/messamat)

# Define utility/generic functions used throughout project and run actual access calculation.

#Structure of analysis
# A. UTILITY FUNCTIONS: generic functions used in this code and rest of project workflow
#    (imported in other scripts within this project as:  from accesscalc_parallel.py import *)
#
# B. ANALYSIS FUNCTIONS: functions to iterate over groups of points and process access index for each
#    group of points and analysis year
#
# C. MAIN ANALYSIS: compute mean access index within livelihood-specific buffer around locations in parallel.
#    To run correctly, this script must be inside a directory with the following structure
#    (here with Charcoal production example to be run for 2000, 2010, and 2018 with three chunks):
#    /data:
#        - /data/Charcoal_production15000_0.gdb: self-contained input gdb to run on one core. 15000 reflects buffer size, 0 reflects chunk number.
#            - /data/Charcoal_production15000_0.gdb/Charcoal_production_bw1_2000: raster of charcoal-specific barriers for 2000
#            - /data/Charcoal_production15000_0.gdb/Charcoal_production_bw1_2010: raster of charcoal-specific barriers for 2010
#            - /data/Charcoal_production15000_0.gdb/Charcoal_production_bw1_2018: raster of charcoal-specific barriers for 2018
#            - /data/Charcoal_production15000_0.gdb/Hansen_GFC_v16_treecover2000: binary raster of forest cover from 2000
#            - /data/Charcoal_production15000_0.gdb/Hansen_GFC_v16_treecover2010: binary raster of forest cover from 2010
#            - /data/Charcoal_production15000_0.gdb/Hansen_GFC_v16_treecover2018: binary raster of forest cover from 2018
#            - /data/Charcoal_production15000_0.gdb/subpointsCharcoal_production15000_0: points to analyze (group numbers in attribute table)
#        - /data/Caprine_livestock10000_1.gdb:
#        - /data/Caprine_livestock10000_2.gdb:
#
#   /results: empty directory where output dbf tables will be written within chunk-specific directory (e.g. /results/Charcoal_production15000_0)
#
#   /src: directory containing this code (accesscalc_parallel.py)
#
# This directory structure is automatically generated by running mergetables_rechunk.py
#
#
# To launch:
#   If python not in environment variables, open terminal as an administrator and type the following lines (adapted to your path):
#   C:
#   cd \Python27\ArcGISx6410.7
#   python C:\Users\olivia_admin\White_input20200518\src\accesscalc_parallel.py
#-----------------------------------------------------------------------------------------------------------------------

import os
import sys
import gc
import traceback
import re
import arcpy
from arcpy.sa import *
from concurrent.futures import TimeoutError
import psutil
from functools import partial
from pebble import ProcessPool

arcpy.env.overwriteOutput = 'True'
arcpy.CheckOutExtension("Spatial")

###---------------------------------------- A. UTILITY FUNCTIONS ----------------------------------------------------###
def getwkspfiles(dir, repattern=None):
    """Get Workspace files.

    Retrieve path for all files in an ArcGIS workspace (file or personal GDB) that match a regex pattern.

    Args:
        dir (str): ArcGIS-compatible workspace to search.
        repattern (str): regex pattern to use in searching for file names.

    Returns:
        list: Full paths to all files matching repattern in dir.
    """
    arcpy.env.workspace = dir
    filenames_list = (arcpy.ListDatasets() or []) + \
                     (arcpy.ListTables() or []) +\
                     (arcpy.ListFeatureClasses() or []) # Either LisDatsets or ListTables may return None so need to create empty list alternative
    if not repattern == None:
        filenames_list = [filen for filen in filenames_list if re.search(repattern, filen)]
    return ([os.path.join(dir, filen) for filen in filenames_list])
    arcpy.ClearEnvironment('workspace')

def getfilelist(dir, repattern=None, gdbf=True, nongdbf=True):
    """Get file list.

    Retrieve path for all files in directory and all subdirectories that match a regex pattern.
    Iteratively goes through all subdirectories inside dir and allows for ArcGIS-compatible workspace search

    Args:
        dir (str): directory to search. All subdirectories will also be searched.
        repattern (str): regex pattern to use in searching for file names.
        gdbf (bool): whether to consider/search ArcGIS workspaces (e.g. file geodatabases)
        nongdbf (bool): whether to consider/search non-ArcGIS workspaces (e.g. files within standard directories)

    Returns:
        list: Full paths to all files matching repattern in dir
    """

    try:
        if arcpy.Describe(dir).dataType == 'Workspace':
            if gdbf == True:
                print('{} is ArcGIS workspace...'.format(dir))
                filenames_list = getwkspfiles(dir, repattern)
            else:
                raise ValueError(
                    "A gdb workspace was given for dir but gdbf=False... either change dir or set gdbf to True")
        else:
            filenames_list = []

            if gdbf == True:
                for (dirpath, dirnames, filenames) in os.walk(dir):
                    for in_dir in dirnames:
                        fpath = os.path.join(dirpath, in_dir)
                        if arcpy.Describe(fpath).dataType == 'Workspace':
                            print('{} is ArcGIS workspace...'.format(fpath))
                            filenames_list.extend(getwkspfiles(dir=fpath, repattern=repattern))

            if nongdbf == True:
                for (dirpath, dirnames, filenames) in os.walk(dir):
                    for file in filenames:
                        if repattern is None:
                            filenames_list.append(os.path.join(dirpath, file))
                        else:
                            if re.search(repattern, file):
                                filenames_list.append(os.path.join(dirpath, file))
        return (filenames_list)

    # Return geoprocessing specific errors
    except arcpy.ExecuteError:
        arcpy.AddError(arcpy.GetMessages(2))
    # Return any other type of error
    except:
        # By default any other errors will be caught here
        e = sys.exc_info()[1]
        print(e.args[0])

def pathcheckcreate(path, verbose=True):
    """Check then create path

    Check whether a directory or ArcGIS workspace path exists. If not, create directory and all other parts of the
    directory tree that are missing.

    Args:
        path (str): path of directory to check and create if non-existent. Can be an ArcGIS file geodatabase.
        verbose (bool): whether to display created directories.

    Returns:
        None. Create directories.
    """

    dirtocreate = []
    # Loop upstream through path to check which directories exist, adding those that don't exist to dirtocreate list
    while not os.path.exists(os.path.join(path)):
        dirtocreate.append(os.path.split(path)[1])
        path = os.path.split(path)[0]

    dirtocreate.reverse()

    # After reversing list, iterate through directories to create starting with the most upstream one
    for dir in dirtocreate:
        # If gdb doesn't exist yet, use arcpy method to create it and then stop the loop to prevent from trying to create anything inside it
        if os.path.splitext(dir)[1] == '.gdb':
            if verbose:
                print('Create {}...'.format(dir))
            arcpy.CreateFileGDB_management(out_folder_path=path,
                                           out_name=dir)
            break

        # Otherwise, if it is a directory name (no extension), make a new directory
        elif os.path.splitext(dir)[1] == '':
            if verbose:
                print('Create {}...'.format(dir))
            path = os.path.join(path, dir)
            os.mkdir(path)

def task_done(future):
    """Check whether timeout or other error occurred in pool support worker.

    Args:
        future (concurrent.futures.Future): encapsulates the asynchronous execution of a callable.
    Returns:
        str: message of error and timeout time
        traceback: error stack
    """
    try:
        future.result()  # blocks until results are ready
    except TimeoutError as error:
        print("Function took longer than %d seconds" % error.args[1])
    except Exception as error:
        print("Function raised %s" % error)
        print(error.traceback)  # traceback of the function

###---------------------------------------- B. ANALYSIS FUNCTIONS ---------------------------------------------------###
def accesscalc(inllhood, ingroup, inpoints, inbuffer_radius, inyears, inbarrierweight_outras,
               inforestyearly, costtab_outdir):
    """
    Access calculation.

    Core function to compute access index for all analysis years for a given livelihood and group of points.

    Args:
        inllhood (str): livelihood to analyze
        ingroup (int): livelihood-specific group identification number to analyze (group of points far enough apart
                       to compute non-overlapping cost distance within livelihood-specific buffer sizes
        inpoints (feature class): locations around which to compute access index for (usually, all points in group)
        inbuffer_radius (int, meters): livelihood-specific length of buffer radius within which to compute access index
        inyears (list of str): years to run analysis for
        inbarrierweight_outras (dictionary): paths to livelihood-specific rasters of weighted barriers for each year
                                            (dictionary key is year)
        inforestyearly (dictionary): paths to binary rasters of forest cover for each year
                                    (dictionary key is year, only useful for Charcoal production livelihood)
        costtab_outdir (str): output directory to write results to.

    Returns:
         None.
         Side effect: create dbf tables, a table for each year of inyears. Each table contains mean access index within inbbufer_radius for every point.
    """

    # Buffer subsetted points based on livelihood-specific buffer distance
    #print('Bufferring...')
    try:
        arcpy.Buffer_analysis(in_features=inpoints,
                              out_feature_class=r'in_memory/subbuffers{}'.format(ingroup),
                              buffer_distance_or_field=inbuffer_radius,
                              dissolve_option='NONE',
                              method='PLANAR')

        templateras = inbarrierweight_outras[inyears[0]]
        arcpy.env.snapRaster = templateras
        buffras_memory = r'in_memory/subbufferas{}'.format(ingroup)
        #print('Conversion to raster...')
        arcpy.FeatureToRaster_conversion(in_features=r'in_memory/subbuffers{}'.format(ingroup),
                                         field='pointid',
                                         out_raster=buffras_memory,
                                         cell_size=templateras)

        # Iterate through years of analysis
        #print('Iterating through years...')
        arcpy.env.mask = buffras_memory
        tempaccessdict = {}
        for year in inyears:
            outtab_year = os.path.join(costtab_outdir, 'CD_{0}_{1}_w1_{2}.dbf'.format(inllhood, year, ingroup))
            dictkey = '{0}{1}'.format(ingroup, year)
            if not arcpy.Exists(outtab_year):
                #print(year)
                # Compute cost distance and access
                if inllhood == 'Charcoal_production':
                    # Round(100*forest resource weighting*(1/(1+cost))
                    tempaccessdict[dictkey] = \
                        Int(100 * Raster(inforestyearly[year]) * \
                            (1 / (1 + CostDistance(in_source_data=inpoints,
                                                   in_cost_raster=Raster(inbarrierweight_outras[year])+0.0001))) +
                            0.5)
                else:
                    # Round(100*(1/(1+cost))
                    tempaccessdict[dictkey] = \
                        Int(100 * \
                            (1 / (1 + CostDistance(in_source_data=inpoints,
                                                   in_cost_raster=Raster(inbarrierweight_outras[year])+0.0001))) +
                            0.5)

                # Zonal statistics based on buffer (using pointid, the unique ID of each point for that livelihood)
                # Compute mean access within livelihood-specific buffer and writes it out to table
                #accessras.save(os.path.join(costtab_outdir, 'test_{0}_{1}_w1_{2}'.format(inllhood, year, ingroup))
                ZonalStatisticsAsTable(in_zone_data=buffras_memory,
                                       zone_field='Value',
                                       in_value_raster=tempaccessdict[dictkey],
                                       out_table=outtab_year,
                                       ignore_nodata='DATA',
                                       statistics_type='MEAN')

            #Clean up
            try:
                arcpy.Delete_management(tempaccessdict[dictkey])
                del tempaccessdict[dictkey]
            except:
                pass

        arcpy.ClearEnvironment("mask")
        arcpy.Delete_management(buffras_memory)
        arcpy.Delete_management("in_memory")
        del tempaccessdict

    except:
        #Clean up memory, environment, and temporary files
        arcpy.ClearEnvironment("mask")
        arcpy.Delete_management("in_memory")

        # Get the traceback object
        tb = sys.exc_info()[2]
        tbinfo = traceback.format_tb(tb)[0]

        # Print Python error messages
        print("PYTHON ERRORS:\nTraceback info:\n" + tbinfo + "\nError Info:\n" + str(sys.exc_info()[1]))
        print("ArcPy ERRORS:\n" + arcpy.GetMessages(2) + "\n")

        try:
            arcpy.Delete_management(tempaccessdict[dictkey])
            del tempaccessdict[dictkey]
        except:
            pass

def accesscalc_chunkedir(inchunkgdb, inyears, outdir):
    """
    Access calculation launching for a given chunk.

    Run access index for groups of points (see accesscalc), all contained within the same geodatabase. This function
    allows for parallelization of access analysis. It defines all paths and iterate through groups.

    Args:
        inchunkgdb (ArcGIS file geodatabase): database that contains groups of points to process
        inyears (list of str): years to run analysis for
        outdir (str): output directory to write results to.

    Returns:
         None
         Run accesscalc function for each group. Create directories containing tables of access index statistics
    """

    arcpy.env.workspace = inchunkgdb
    arcpy.env.scratchWorkspace = 'in_memory'

    rootname = os.path.splitext(os.path.split(inchunkgdb)[1])[0]
    llhood = re.search('[aA-zZ]*[_][aA-zZ]*', rootname).group()
    bufferrad = re.search('[0-9]*(?=_[0-9]*$)', rootname).group()
    inpoints = 'subpoints{}'.format(rootname)
    barrierweight_outras = {year: '{0}_bw1_{1}'.format(llhood, year) for year in inyears}
    forestyearly = {year: 'Hansen_GFC_v16_treecover{}'.format(year) for year in inyears}
    print(rootname)

    outdir_chunk = os.path.join(outdir, rootname)
    if not os.path.exists(outdir_chunk):
        os.mkdir(outdir_chunk)

    if 'group{}'.format(llhood) not in [i.name for i in arcpy.Describe(inpoints).indexes]:
        arcpy.AddIndex_management(inpoints, fields='group{}'.format(llhood),
                                  index_name='group{}'.format(llhood))  # Add index to speed up querying

    # Iterate through groups
    grouplist = {row[0] for row in arcpy.da.SearchCursor(inpoints, 'group{}'.format(llhood))}
    for group in grouplist:
        #If all output tables don't already exist
        if not all(arcpy.Exists(os.path.join(outdir_chunk, 'CD_{0}_{1}_w1_{2}.dbf'.format(llhood, year, group)))
                   for year in inyears):
            print(group)
            #tic = time.time()
            arcpy.MakeFeatureLayer_management(in_features=inpoints, out_layer='pointslyr_{}'.format(group),
                                              where_clause='{0} = {1}'.format('group{}'.format(llhood), group))
            accesscalc(inllhood=llhood,
                       ingroup=group,
                       inpoints='pointslyr_{}'.format(group),
                       inbuffer_radius=bufferrad,
                       inyears=inyears,
                       inbarrierweight_outras=barrierweight_outras,
                       inforestyearly=forestyearly,
                       costtab_outdir=outdir_chunk)

            arcpy.Delete_management('pointslyr_{}'.format(group))
            #print(time.time() - tic)
        else:
            print('Group {} was already analyzed...'.format(group))

        gc.collect()

    del grouplist
    arcpy.Delete_management("in_memory")
    arcpy.ClearEnvironment('workspace')

###---------------------------------------- C. MAIN ANALYSIS ---------------------------------------------------###
if __name__ == '__main__':
    #Define directory structure
    formatdir = os.path.join(os.path.dirname(os.path.abspath(__file__)).split('\\src')[0])
    datadir = os.path.join(formatdir, 'data')
    resdir = os.path.join(formatdir, 'results')
    arcpy.env.workspace = datadir
    ingdbs = arcpy.ListWorkspaces('*', workspace_type='FileGDB')

    #---- Run in parallel for all gdb -----#
    # Define years of analysis (improve with argpars, etc. down the line)
    if len(sys.argv) == 2:     # Try to get years to process from command line input
        analysis_years = sys.argv[1]

        # Make sure analysis_years is list of strings
        if isinstance(analysis_years, str) or isinstance(analysis_years, int):
            analysis_years = [analysis_years]
        analysis_years = [str(yr) for yr in analysis_years]

    else: #Otherwise, default to
        analysis_years = ['2000', '2010', '2018']

    print('Processing {}...'.format(", ".join(analysis_years)))

    #Get number of cpus to run unto - should work on HPC and personal computer
    ncpus = int(os.environ.get('SLURM_CPUS_PER_TASK')) if \
        os.environ.get('SLURM_CPUS_PER_TASK') is not None else \
        psutil.cpu_count(logical=False)

    maxruntime = 7*24*3600  # Define maximum running time of worker processes (useful if running on HPC with limited availability)

    print('Launch parallel processing on {0} cores with {1}s timeout...'.format(ncpus, maxruntime))
    with ProcessPool(max_workers=ncpus) as pool:  # Create pool of worker processes (with N # of physical cores)
        #Assign chunked gdbs to worker processes, keeping analysis years and outstats arguments constant with 'partial'
        future = pool.map(partial(accesscalc_chunkedir, inyears=analysis_years, outdir=resdir),
                          ingdbs,
                          #chunksize=int(0.5 + len(ingdbs) / float(ncpus)),  # for HPC ---- Divide all gdbs among chunks upfront so that timeout doesn't lead to new worker process
                          timeout=maxruntime) #Raise timeout error after maxruntime
        task_done(future) #Return timeout error